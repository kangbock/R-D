# CI/CDì™€ PLG ì‹¤ìŠµ ê³¼ì •



## ì‚¬ì „ ì‘ì—… - Terraform ì¸í”„ë¼ êµ¬ì„±

Terraformì„ ì‚¬ìš©í•˜ì—¬ ì¸í”„ë¼ë¥¼ ì½”ë“œë¡œ ê´€ë¦¬í•˜ê³  ìë™í™”í•©ë‹ˆë‹¤.

- GitHub Repository: [Terraform Basic](https://github.com/kangbock/terraform-basic)
<br><br>



## CI/CD 

ì§€ì†ì  í†µí•©ê³¼ ë°°í¬ ìë™í™”ë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ì„ Jenkins, Kaniko, Harbor, ArgoCDë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
<br>

**Workflow**
![alt text](img/image.png)
<br>

- Jenkinsë¡œ ì†ŒìŠ¤ì½”ë“œ í†µí•© ë¹Œë“œ
- Kanikoë¡œ Docker ì´ë¯¸ì§€ ìƒì„± í›„ Harborì— ì´ë¯¸ì§€ í‘¸ì‹œ
- ArgoCDê°€ ë³€ê²½ëœ ë°°í¬ YAMLì„ í†µí•´ ìë™ ë°°í¬ ìˆ˜í–‰
- Slackìœ¼ë¡œ ìƒíƒœ ì•Œë¦¼
<br><br>



## PLG

ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì¸í”„ë¼ì˜ ë©”íŠ¸ë¦­ ë° ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ê´€ì°°ì„±ì„ ë†’ì´ê³  ë¶„ì„í•©ë‹ˆë‹¤.
<br>

### Prometheus Workflow

ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì•Œë¦¼ ë°œì†¡
<br>

![alt text](img/image-2.png)
<br>

### Grafana Loki Workflow

ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„
<br>

![alt text](img/image-3.png)
<br><br>



## Azure Login

Azure VM(Ubuntu)ì—ì„œ Docker, Azure CLI ë° Kubernetes CLI ì„¤ì • ë° AKS ì—°ê²°
<br>

**Azure VM (Ubuntu 22.04)**
```
sudo apt update 
sudo apt-get upgrade -y
sudo apt-get install -y docker.io 
sudo git clone https://github.com/kangbock/msa_nginx.git 
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
az login --service-principal --username $APPID --password $APPPW --tenant $TENANT --output table
az account set --subscription kblee_cc1_gp_mpn_2-cloudsecurity-02
az aks install-cli
az aks get-credentials --resource-group poc-rg --name poc-test-aks
```
<br>

**Helm Install**

Helmì„ ì´ìš©í•˜ì—¬ Kubernetes ì•± ë°°í¬ ë° ê´€ë¦¬
<br>

```
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
```
<br><br>



## Cert-manager

ì¸ì¦ì„œ ìë™ ë°œê¸‰ ë° ê´€ë¦¬ (LetsEncrypt ì—°ë™)
<br>

```
kubectl create namespace cert-manager

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

# Install the cert-manager Helm chart
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version=v1.8.0 \
  --set installCRDs=true \
  --set nodeSelector."kubernetes\.io/os"=linux
```
<br><br>



## Harbor

í”„ë¼ì´ë¹— ì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì¶•

- NGINX Ingress Controller ë° SSL ì ìš©
- Cert-managerë¡œ ìë™ ì¸ì¦ì„œ ê´€ë¦¬
<br>

```
kubectl create ns devops-tools
kubectl apply -f R-D/harbor/harbor-certificate.yaml
kubectl get nodes --show-labels | grep linux
```
<br>

**Cluster Issuer**
```
kubectl apply -f cert-manager/.
```
<br>

**Harbor Deploy**
```
# nginx ingress controller install
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

helm install ingress-nginx ingress-nginx/ingress-nginx \
  --set controller.service.annotations."service\.beta\.kubernetes\.io/azure-load-balancer-health-probe-request-path"=/healthz

# repo ë“±ë¡
helm repo add harbor https://helm.goharbor.io

# ì••ì¶•íŒŒì¼ ë‹¤ìš´ë¡œë“œ
helm fetch harbor/harbor --untar

# env
sed -i 's/core.harbor.domain/harbor.k-tech.cloud/g' ./harbor/values.yaml
sed -i 's/className: ""/className: "nginx"/g' ./harbor/values.yaml
sed -i '19 s/certSource: auto/certSource: secret/g' ./harbor/values.yaml
sed -i '28 s/secretName: ""/secretName: harbor-secret/g' ./harbor/values.yaml
sed -i '/# for Envoy/a\      certmanager.k8s.io/disable-auto-restart: "true"' ./harbor/values.yaml
sed -i '/# for Envoy/a\      cert-manager.io/cluster-issuer: letsencrypt' ./harbor/values.yaml
# sed -i '/# for Envoy/a\      cert-manager.io/cluster-issuer: letsencrypt-staging' ./harbor/values.yaml

# harbor deploy
helm install harbor -f ./harbor/values.yaml ./harbor/. -n devops-tools

# Harbor login
# ID : admin
# PW : Harbor12345
```
<br>

![alt text](img/image-12.png)
<br><br>



## Istio Service Mesh

ì„œë¹„ìŠ¤ ê°„ í†µì‹  ë³´ì•ˆê³¼ ê´€ë¦¬

- ìë™ ì‚¬ì´ë“œì¹´ ì£¼ì…
- Kialië¥¼ í†µí•œ ê°€ì‹œì„± í™•ë³´
<br>

**Isito Download**
```
curl -L https://istio.io/downloadIstio | sh -
mv istio-1.24.2 istio
cd istio
export PATH=$PWD/bin:$PATH
cd ..
```
<br>

**Istio Deploy**
```
istioctl install --set profile=demo -y
# istioctl install --set profile=demo --skip-confirmation
```
<br>

**auto sidecar injection**
```
kubectl label namespace default istio-injection=enabled --overwrite

kubectl get namespace -L istio-injection
```
<br>

**sidecar check**
```
istioctl experimental check-inject <pod-name>
```
<br><br>

## Jenkins

**Deploy**
```
kubectl apply -f R-D/jenkins/.
```
<br>

**Password**
```
kubectl exec -it svc/jenkins-service -n devops-tools -- cat /var/jenkins_home/secrets/initialAdminPassword
```
<br>

<aside class="warning">ğŸ’¡ í”ŒëŸ¬ê·¸ì¸ ê´€ë¦¬ â†’ kubernetes, slack notification ì„¤ì¹˜</aside><br>

<aside class="warning">ğŸ’¡ ì‹œìŠ¤í…œ ì„¤ì • â†’ GitHub Server, slack ì—°ê²°</aside><br>

<aside class="warning">ğŸ’¡ Node ê´€ë¦¬ â†’ Clouds â†’ New Cloud â†’ WebSocket Check</aside><br><br>

### Kaniko

ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œë¥¼ ìœ„í•œ ë³´ì•ˆ ê°•í™” ë„êµ¬

- Docker ë°ëª¬ ì—†ì´ Kubernetes ë‚´ë¶€ ë¹Œë“œ ê°€ëŠ¥
<br>

#### Docker vs Kaniko
**Docker**Â : DockerëŠ” Docker ë°ëª¬ì´ í˜¸ìŠ¤íŠ¸ ì‹œìŠ¤í…œì—ì„œ ì‹¤í–‰ë˜ê³  ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ëŠ” ë°ëª¬ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.Â ì´ë¥¼ ìœ„í•´ì„œëŠ” íŠ¹íˆ Kubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ ë³´ì•ˆ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ê¶Œí•œ ìˆëŠ” ì•¡ì„¸ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.

**Kaniko**Â : KanikoëŠ” ì»¨í…Œì´ë„ˆ ë˜ëŠ” Kubernetes í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ Dockerfileì—ì„œ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.Â íŠ¹ë³„í•œ ê¶Œí•œì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Kubernetes í™˜ê²½ì˜ ë³´ì•ˆì´ ë”ìš± ê°•í™”ë©ë‹ˆë‹¤.
<br>

**Create secret**
```
docker login https://harbor.k-tech.cloud
cat ~/.docker/config.json
cat ~/.docker/config.json | base64
```
<br>

**regcred.yaml**
```
apiVersion: v1
kind: Secret
metadata:
  name: docker-config-secret
  namespace: default
data:
  .dockerconfigjson: ì¸ì½”ë”©í•œ ë°ì´í„°
type: kubernetes.io/dockerconfigjson
```

```
kubectl apply -f R-D/kaniko/.
kubectl apply -n devops-tools -f R-D/kaniko/.
kubectl apply -n istio-system -f R-D/kaniko/.
```
<br>

### Jenkins Pipeline êµ¬ì„±

CI/CD ìë™í™”ë¥¼ ìœ„í•œ Jenkins Pipeline

- Kanikoë¥¼ ì´ìš©í•œ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œ
- GitOpsë¥¼ í†µí•œ ìë™ ë°°í¬
- Slack ì•Œë¦¼ ì—°ë™
<br>

**Configuration**
<aside class="warning">ğŸ’¡ check : Do not allow the pipeline to resume if the controller restarts</aside><br>

<aside  class="warning">ğŸ’¡ check : GitHub hook trigger for GITScm polling</aside><br><br>

**Frontend**
```
podTemplate(yaml: '''
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: alpine
        image: alpine/git:latest
        command:
        - sleep
        args:
        - 99d
      - name: kaniko
        image: gcr.io/kaniko-project/executor:debug
        command:
        - sleep
        args:
        - 99d
        volumeMounts:
        - name: kaniko-secret
          mountPath: /kaniko/.docker
      restartPolicy: Never
      volumes:
      - name: kaniko-secret
        secret:
          secretName: docker-config-secret
          items:
            - key: .dockerconfigjson
              path: config.json
''') {

    node(POD_LABEL) {
        try {
            stage("Set Variable") {
                git url: 'https://github.com/kangbock/msa_nginx.git', branch: 'main'
                script(){
                    env.GIT_COMMIT = sh(script: "git rev-parse HEAD", returnStdout: true).trim()
                    GIT_TAG = sh (script: 'git describe --always', returnStdout: true).trim();
                    SLACK_CHANNEL = "#devops";
                    SLACK_SUCCESS_COLOR = "#2C953C";
                    SLACK_FAIL_COLOR = "#FF3232";
                    // Git Commit ê³„ì •
                    GIT_COMMIT_AUTHOR = sh(script: "git --no-pager show -s --format=%an ${env.GIT_COMMIT}", returnStdout: true).trim();
                    // Git Commit ë©”ì‹œì§€
                    GIT_COMMIT_MESSAGE = sh(script: "git --no-pager show -s --format=%B ${env.GIT_COMMIT}", returnStdout: true).trim();
                }
            }
            slackSend (
                channel: SLACK_CHANNEL,
                color: SLACK_SUCCESS_COLOR,
                message: "========================================\n The ${env.JOB_NAME}(${env.BUILD_NUMBER}) pipeline has started.\n\n          Author : ${GIT_COMMIT_AUTHOR} \n          Commit Message : ${GIT_COMMIT_MESSAGE}\n\n${env.BUILD_URL}"
            )
        } catch(git) {
                slackSend (
                    channel: SLACK_CHANNEL,
                    color: SLACK_SUCCESS_COLOR,
                    message: "========================================\n${env.JOB_NAME}(${env.BUILD_NUMBER}) pipeline failed.\n\n          Author : ${GIT_COMMIT_AUTHOR} \n          Commit Message : ${GIT_COMMIT_MESSAGE}\n\n${env.BUILD_URL}"
                )
            throw git;
        }
        

        stage('Kaniko Build and Push') {
            git url: 'https://github.com/kangbock/msa_nginx.git', branch: 'main'
            
            container('kaniko') {
                try {
                    stage('nginx') {
                        sh '/kaniko/executor -f `pwd`/Dockerfile -c `pwd` --insecure --cache=true --destination=harbor.k-tech.cloud/msa/nginx:${BUILD_NUMBER}'
                    }
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_SUCCESS_COLOR,
                        message: "NGINX Image Deployment and Push Successfully."
                    )
                } catch(Build) {
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_FAIL_COLOR,
                        message: "Image deployment and push failed."
                    )
                    throw Build;
                }
            }
            
            git url: 'https://github.com/kangbock/msa_deploy.git', branch: 'main'
            container('alpine') {
                try {
                    stage('GitHub Push') {
                        sh 'sed -i s/nginx:.*/nginx:${BUILD_NUMBER}/g ./nginx.yaml'
		                    sh 'git config --global --add safe.directory /home/jenkins/agent/workspace/nginx'
		                    sh 'git config --global user.email \'kangbock@naver.com\''
		                    sh 'git config --global user.name \'kangbock\''
		                    sh 'git config --global --add safe.directory /home/jenkins/agent/workspace/main'
		                    sh 'git add ./'
		                    sh 'git commit -a -m "updated the image tag to ${BUILD_NUMBER}" || true'
		                    sh 'git remote add kb97 https://<GIT_TOKEN>@github.com/kangbock/msa_deploy.git'
		                    sh 'git push -u kb97 main'
                    }
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_SUCCESS_COLOR,
                        message: "Deployment was successful.\n========================================"
                    )
                } catch(push) {
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_FAIL_COLOR,
                        message: "Deployment failed.\n========================================"
                    )
                    throw push;
                }
            }
        }
    }
}
```
<br>

**Backend (Login)**
```
podTemplate(yaml: '''
    apiVersion: v1
    kind: Pod
    metadata:
      name: login
    spec:
      containers:
      - name: alpine
        image: alpine/git:latest
        command:
        - sleep
        args:
        - 99d
      - name: kaniko
        image: gcr.io/kaniko-project/executor:debug
        command:
        - sleep
        args:
        - 99d
        volumeMounts:
        - name: kaniko-secret
          mountPath: /kaniko/.docker
      restartPolicy: Never
      volumes:
      - name: kaniko-secret
        secret:
          secretName: docker-config-secret
          items:
            - key: .dockerconfigjson
              path: config.json
''') {

    node(POD_LABEL) {
        try {
            stage("Set Variable") {
                git url: 'https://github.com/kangbock/msa_loginjs.git', branch: 'main'
                script(){
                    env.GIT_COMMIT = sh(script: "git rev-parse HEAD", returnStdout: true).trim()
                    GIT_TAG = sh (script: 'git describe --always', returnStdout: true).trim();
                    SLACK_CHANNEL = "#devops";
                    SLACK_SUCCESS_COLOR = "#2C953C";
                    SLACK_FAIL_COLOR = "#FF3232";
                    // Git Commit ê³„ì •
                    GIT_COMMIT_AUTHOR = sh(script: "git --no-pager show -s --format=%an ${env.GIT_COMMIT}", returnStdout: true).trim();
                    // Git Commit ë©”ì‹œì§€
                    GIT_COMMIT_MESSAGE = sh(script: "git --no-pager show -s --format=%B ${env.GIT_COMMIT}", returnStdout: true).trim();
                }
            }
            slackSend (
                channel: SLACK_CHANNEL,
                color: SLACK_SUCCESS_COLOR,
                message: "========================================\n The ${env.JOB_NAME}(${env.BUILD_NUMBER}) pipeline has started.\n\n          Author : ${GIT_COMMIT_AUTHOR} \n          Commit Message : ${GIT_COMMIT_MESSAGE}\n\n${env.BUILD_URL}"
            )
        } catch(git) {
                slackSend (
                    channel: SLACK_CHANNEL,
                    color: SLACK_SUCCESS_COLOR,
                    message: "========================================\n${env.JOB_NAME}(${env.BUILD_NUMBER}) pipeline failed.\n\n          Author : ${GIT_COMMIT_AUTHOR} \n          Commit Message : ${GIT_COMMIT_MESSAGE}\n\n${env.BUILD_URL}"
                )
            throw git;
        }
        

        stage('Kaniko Build and Push') {
            git url: 'https://github.com/kangbock/msa_loginjs.git', branch: 'main'
            
            container('kaniko') {
                try {
                    stage('login') {
                        sh '/kaniko/executor -f `pwd`/Dockerfile -c `pwd` --insecure --cache=true --destination=harbor.k-tech.cloud/msa/login_js:${BUILD_NUMBER}'
                    }
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_SUCCESS_COLOR,
                        message: "LOGIN Image Deployment and Push Successfully."
                    )
                } catch(Build) {
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_FAIL_COLOR,
                        message: "Image deployment and push failed."
                    )
                    throw Build;
                }
            }
            
            git url: 'https://github.com/kangbock/msa_deploy.git', branch: 'main'
            container('alpine') {
                try {
                    stage('GitHub Push') {
                        sh 'sed -i s/login_js:.*/login_js:${BUILD_NUMBER}/g ./login_js.yaml'
		                    sh 'git config --global --add safe.directory /home/jenkins/agent/workspace/login'
		                    sh 'git config --global user.email \'kangbock@naver.com\''
		                    sh 'git config --global user.name \'kangbock\''
		                    sh 'git config --global --add safe.directory /home/jenkins/agent/workspace/main'
		                    sh 'git add ./'
		                    sh 'git commit -a -m "updated the image tag to ${BUILD_NUMBER}" || true'
		                    sh 'git remote add kb97 https://<GIT_TOKEN>@github.com/kangbock/msa_deploy.git'
		                    sh 'git push -u kb97 main'
                    }
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_SUCCESS_COLOR,
                        message: "Deployment was successful.\n========================================"
                    )
                } catch(push) {
                    slackSend (
                        channel: SLACK_CHANNEL,
                        color: SLACK_FAIL_COLOR,
                        message: "Deployment failed.\n========================================"
                    )
                    throw push;
                }
            }
        }
    }
}
```
<br>

![alt text](img/image-13.png)
<br><br>

### Slack Notification

Jenkinsì™€ Slackì„ ì—°ë™í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì‹¤ì‹œê°„ ì•Œë¦¼
<br>

**https://ì›Œí¬ìŠ¤í˜ì´ìŠ¤.slack.com/apps** ì— ì ‘ì†í•˜ì—¬ **Jenkins Ci ì•±** ì„¤ì¹˜<br>
Jenkins Ci ì„¤ì • ì§€ì¹¨ ë‹¨ê³„ì— ë”°ë¼ êµ¬ì„±

![alt text](img/image-1.png)

![alt text](img/image-15.png)
<br><br>

## ArgoCD

GitOps ê¸°ë°˜ ì§€ì†ì  ë°°í¬ ê´€ë¦¬

- ìë™ ë™ê¸°í™” ë° Slack ì•Œë¦¼ ì—°ë™
<br>

### ArgoCD Deploy
```
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
kubectl apply -f R-D/argocd/.
```
<br>

```
# ì•”í˜¸ ì°¾ê¸°
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
```
<br>

![alt text](img/image-14.png)
<br>

### ArgoCD Notification

ArgoCD ìƒíƒœ ë³€ê²½ ì‹œ Slack ì•Œë¦¼
<br>

**Create ArgoCD App**

https://api.slack.com/apps

![alt text](img/image-5.png)
![alt text](img/image-6.png)
<br>

**OAuth & Permissions Bot Token Scopes Setting**

![alt text](img/image-7.png)
![alt text](img/image-8.png)
<br>

**argocd-notification-values.yaml**
```
notifications:
  enabled: true
  argocdUrl: "https://argocd.k-tech.cloud"

  secret:
    create: true
    items:
      slack-token: "<SLACK_TOKEN>"

  notifiers:
    service.slack: |-
      token: $slack-token

  templates:
    # 1) ì•± Health Degraded
    template.app-health-degraded: |-
      slack:
        text: " "
        attachments: |-
          [{
            "color": "danger",
            "text": "<!channel>\n\n:rotating_light: *{{ .app.metadata.name }}* ìƒíƒœê°€ *Degraded*.",
            "fields": [
              { "title": "Sync Status",
                "value": "{{ .app.status.sync.status | default "Unknown" }}",
                "short": true },
              { "title": "Health",
                "value": "{{ .app.status.health.status | default "Unknown" }}",
                "short": true },
              { "title": "Repository",
                "value": "{{ .app.spec.source.repoURL }}",
                "short": false },
              { "title": "ArgoCD URL",
                "value": "<{{ .context.argocdUrl }}/applications/{{ .app.metadata.name }}?operation=true>",
                "short": false }
            ]
          }]

    # 2) Sync ì§„í–‰ ì¤‘
    template.app-sync-running: |-
      slack:
        text: " "
        attachments: |-
          [{
            "color": "#439FE0",
            "text": "<!channel>\n\n:hourglass_flowing_sand: *{{ .app.metadata.name }}* ë™ê¸°í™” ì§„í–‰ì¤‘.\n\nStarted Time : {{ .app.status.operationState.startedAt }}",
            "fields": [
              { "title": " ", "value": " ", "short": false },
              { "title": "Sync Status",
                "value": "{{ .app.status.sync.status | default "Unknown" }}",
                "short": true },
              { "title": "Repository",
                "value": "{{ .app.spec.source.repoURL }}",
                "short": true },
              { "title": "ArgoCD URL",
                "value": "<{{ .context.argocdUrl }}/applications/{{ .app.metadata.name }}?operation=true>",
                "short": false }
            ]
          }]

    # 3) Sync ì„±ê³µ
    template.app-sync-succeeded: |-
      slack:
        text: " "
        attachments: |-
          [{
            "color": "good",
            "text": "<!channel>\n\n:white_check_mark: *{{ .app.metadata.name }}* ë™ê¸°í™” ì„±ê³µ!\n\nFinished Time : {{ .app.status.operationState.finishedAt }}.",
            "fields": [
              { "title": "Sync Status",
                "value": "{{ .app.status.sync.status | default "Synced" }}",
                "short": true },
              { "title": "Health",
                "value": "{{ .app.status.health.status | default "Unknown" }}",
                "short": true },
              { "title": "Repository",
                "value": "{{ .app.spec.source.repoURL }}",
                "short": false },
              { "title": "Revision",
                "value": "{{ .app.status.sync.revision | default "-" }}",
                "short": false },
              { "title": "ArgoCD URL",
                "value": "<{{ .context.argocdUrl }}/applications/{{ .app.metadata.name }}?operation=true>",
                "short": false }
            ]
          }]

    # 4) Sync ì‹¤íŒ¨
    template.app-sync-failed: |-
      slack:
        text: " "
        attachments: |-
          [{
            "color": "danger",
            "text": "<!channel>\n\n:x: *{{ .app.metadata.name }}* ë™ê¸°í™” ì‹¤íŒ¨!!!\n\nFinished Time : {{ .app.status.operationState.finishedAt }}.",
            "fields": [
              { "title": "Phase",
                "value": "{{ .app.status.operationState.phase | default "Error" }}",
                "short": true },
              { "title": "Health",
                "value": "{{ .app.status.health.status | default "Unknown" }}",
                "short": true },
              { "title": "Repository",
                "value": "{{ .app.spec.source.repoURL }}",
                "short": false },
              { "title": "Message",
                "value": "{{ .app.status.operationState.message | default "N/A" }}",
                "short": false },
              { "title": "ArgoCD URL",
                "value": "<{{ .context.argocdUrl }}/applications/{{ .app.metadata.name }}?operation=true>",
                "short": false }
            ]
          }]

  triggers:
    # ì ‘ë‘ì‚¬ ì¶”ê°€
    trigger.on-health-degraded: |-
      - when: app.status.health.status == 'Degraded'
        send: [app-health-degraded]

    trigger.on-sync-running: |-
      - when: app.status.operationState.phase == 'Running'
        send: [app-sync-running]

    trigger.on-sync-succeeded: |-
      - when: app.status.operationState.phase == 'Succeeded' && app.status.health.status == 'Healthy'
        send: [app-sync-succeeded]

    trigger.on-sync-failed: |-
      - when: app.status.operationState.phase in ['Error','Failed']
        send: [app-sync-failed]

  subscriptions:
    - recipients:
        - slack:devops
      triggers:
        - on-health-degraded
        - on-sync-running
        - on-sync-succeeded
        - on-sync-failed
```
<br>

**Argocd Notifiaction CM Depoly**
```
helm upgrade argocd argo/argo-cd -n argocd -f argocd-notification-values.yaml --debug --wait --timeout 10m
```
<br>

**Slack**

![alt text](img/image-9.png)
<br><br>

## Prometheus

ë©”íŠ¸ë¦­ ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ê´€ë¦¬

- ì´ìƒ ë°œìƒ ì‹œ Slack ì•Œë¦¼
<br>

### Prometheus Stack
```
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
git clone https://github.com/prometheus-community/helm-charts.git
```
<br>

```
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --debug --timeout 10m -n monitoring
kubectl apply -f R-D/prometheus/.
```
<br>

**Uninstall Command**
```
helm uninstall prometheus
kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
kubectl delete crd alertmanagers.monitoring.coreos.com
kubectl delete crd podmonitors.monitoring.coreos.com
kubectl delete crd probes.monitoring.coreos.com
kubectl delete crd prometheusagents.monitoring.coreos.com
kubectl delete crd prometheuses.monitoring.coreos.com
kubectl delete crd prometheusrules.monitoring.coreos.com
kubectl delete crd scrapeconfigs.monitoring.coreos.com
kubectl delete crd servicemonitors.monitoring.coreos.com
kubectl delete crd thanosrulers.monitoring.coreos.com
```
<br>

### PodMonitor
**Istio â€” ì»¨íŠ¸ë¡¤ í”Œë ˆì¸/ê²Œì´íŠ¸ì›¨ì´**
```
# istio-envoy
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-envoy
  namespace: monitoring
  labels: { release: kube-prometheus-stack }
spec:
  namespaceSelector:
    matchNames: ["default"]
  selector:
    matchExpressions:
      - key: app
        operator: In
        values: ["nginx","login-js"]
  podMetricsEndpoints:
    - port: http-envoy-prom
      path: /stats/prometheus
      interval: 30s
---
# istiod (ì»¨íŠ¸ë¡¤ í”Œë ˆì¸)
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istiod
  namespace: monitoring
  labels: { release: kube-prometheus-stack }
spec:
  jobLabel: app
  namespaceSelector:
    matchNames: ["istio-system"]
  selector:
    matchLabels:
      app: istiod
  podMetricsEndpoints:
    - portNumber: 15014   # istiod monitoring port
      path: /metrics
      interval: 30s
---
# Istio ê²Œì´íŠ¸ì›¨ì´(ingress/egress ê³µí†µ) - Envoy ë©”íŠ¸ë¦­
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-gateways
  namespace: monitoring
  labels: { release: kube-prometheus-stack }
spec:
  jobLabel: istio
  namespaceSelector:
    matchNames: ["istio-system"]
  selector:
    matchExpressions:
      - key: istio               # ì¼ë°˜ì ìœ¼ë¡œ istio=ingressgateway/egressgateway ë¼ë²¨ ì¡´ì¬
        operator: In
        values: ["ingressgateway","egressgateway"]
  podMetricsEndpoints:
    - port: http-envoy-prom
      path: /stats/prometheus
      interval: 30s
```
<br>

**Jenkins (API Token ë°œê¸‰)**
```
apiVersion: v1
kind: Secret
metadata:
  name: jenkins-metrics-basic-auth
  namespace: monitoring
type: Opaque
stringData:
  username: "kangbock"            # Jenkins ì‚¬ìš©ìëª…
  password: "<JENKINS_API_TOKEN>" # ìœ„ì—ì„œ ë°œê¸‰ë°›ì€ í† í°
```
```
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: jenkins
  namespace: monitoring
  labels:
    release: kube-prometheus-stack   # kps ë¦´ë¦¬ìŠ¤ëª…ì— ë§ê²Œ
spec:
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames:
      - devops-tools                 # Jenkinsê°€ ìˆëŠ” NS
  selector:
    matchLabels:
      app: jenkins-server            # ë„ˆì˜ í¬ë“œ ë¼ë²¨
  podMetricsEndpoints:
    - port: httpport                 # ì»¨í…Œì´ë„ˆ í¬íŠ¸ ì´ë¦„(8080)
      path: /prometheus              # í”„ë¦¬í”½ìŠ¤ ìˆìœ¼ë©´ /<prefix>/prometheus
      interval: 30s
      scheme: http
      basicAuth:
        username:
          name: jenkins-metrics-basic-auth
          key: username
        password:
          name: jenkins-metrics-basic-auth
          key: password
```
<br>

**ArgoCD**
```
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: argocd-repo-server
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames: ["argocd"]
  selector:
    matchLabels:
      app.kubernetes.io/name: argocd-repo-server
  podMetricsEndpoints:
    - portNumber: 8084
      path: /metrics
      interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: argocd-server
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames: ["argocd"]
  selector:
    matchLabels:
      app.kubernetes.io/name: argocd-server
  podMetricsEndpoints:
    - portNumber: 8083
      path: /metrics
      interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: argocd-application-controller
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames: ["argocd"]
  selector:
    matchLabels:
      app.kubernetes.io/name: argocd-application-controller
  podMetricsEndpoints:
    - portNumber: 8082
      path: /metrics
      interval: 30s
---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: argocd-applicationset-controller
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames: ["argocd"]
  selector:
    matchLabels:
      app.kubernetes.io/name: argocd-applicationset-controller
  podMetricsEndpoints:
    - portNumber: 8080   # ë°°í¬ì— ë”°ë¼ 8085 ë“±ì¼ ìˆ˜ ìˆìŒ
      path: /metrics
      interval: 30s
```
<br>

**Cert-Manager**
```
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: cert-manager
  namespace: monitoring
  labels: { release: kube-prometheus-stack }
spec:
  jobLabel: app.kubernetes.io/name
  namespaceSelector:
    matchNames: ["cert-manager"]
  selector:
    matchLabels:
      app.kubernetes.io/name: cert-manager
  podMetricsEndpoints:
    - portNumber: 9402
      path: /metrics
      interval: 30s
```
**PodMonitor Target**
<br>

![alt text](img/image-16.png)
<br><br>

### Alertmanager
**Helm Chart Value.yaml**
```
alertmanager:
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/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'
      http_config:
        follow_redirects: true
    inhibit_rules:
      - source_matchers:
          - 'severity = critical'
        target_matchers:
          - 'severity =~ warning|info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'severity = warning'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'alertname = InfoInhibitor'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
    route:
      group_by: ['alertname','namespace','pod']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'slack-notifications'
      routes:
      - receiver: 'slack-notifications'
        matchers:
          - alertname =~ "InfoInhibitor|Watchdog"
    receivers:
    - name: 'slack-notifications'
      slack_configs:
      - channel: '#devops'
        api_url: 'https://hooks.slack.com/services/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'
        username: 'alertmanager'
        title: '{{ .CommonLabels.alertname }} ({{ .Status }})'
        title_link: 'http://alertmanager.k-tech.cloud/#/alerts?receiver=slack-notifications'
        text: |-
          <!channel>
          *Alert:* {{ .CommonLabels.alertname }} | *Status:* {{ .Status }} | *Severity:* {{ or .CommonLabels.severity "n/a" }}

          *Summary:* {{ or .CommonAnnotations.summary .CommonAnnotations.message }}
          *Description:* {{ .CommonAnnotations.description }}
          {{ range .Alerts }}
          *Affected:*
          namespace  = {{ or .Labels.namespace "n/a" }}
          pod               = {{ or .Labels.pod "n/a" }}
          instance       = {{ or .Labels.instance "n/a" }}
          job                = {{ or .Labels.job "n/a" }}

          Started At {{ .StartsAt | tz "Asia/Seoul" | date "2006-01-02 15:04:05 KST" }}
          {{ if eq .Status "resolved" }}Ended At {{ .EndsAt | tz "Asia/Seoul" | date "2006-01-02 15:04:05 KST" }}
          Active For {{ .EndsAt.Sub .StartsAt | humanizeDuration }}
          {{ else }}Active For {{ since .StartsAt | humanizeDuration }}
          {{ end }}
          {{ end }}
          *Links:* https://alertmanager.k-tech.cloud
          Generator: https://prometheus.k-tech.cloud/alerts
        send_resolved: true
```
<br>

**Prometheus Stack Update**
```
helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack --debug -n monitoring -f alertmamager-values.yaml
kubectl apply -f R-D/alertmanager/.
```
<br>

**Slack Notification**<br>

![alt text](img/image-4.png)
<br><br>

## Grafana
ë©”íŠ¸ë¦­ ë° ë¡œê·¸ ì‹œê°í™”ì™€ ë¶„ì„

- Prometheus ë° Loki ë°ì´í„° ì‹œê°í™”
<br>

```
kubectl apply -f R-D/grafana/.
```
<br>

**Grafana admin password**
```
kubectl get secret --namespace monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
```
<br>

![alt text](img/image-10.png)
<br><br>

### Kiali
**kiali.yaml**
```
data:
  config.yaml: |
    external_services:
      prometheus:
        url: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
      grafana:
        external_url: http://kube-prometheus-stack-grafana.monitoring.svc.cluster.local:80
        internal_url: http://kube-prometheus-stack-grafana.monitoring.svc.cluster.local:80
```
<br>

```
kubectl apply -f istio/samples/addons/kiali.yaml
kubectl apply -f R-D/kiali/.
```
<br>

![alt text](img/image-11.png)
<br><br>

### Tempo
ë¶„ì‚° íŠ¸ë ˆì´ì‹± ë°±ì—”ë“œ(ì‹œìŠ¤í…œ)ìœ¼ë¡œ ë ˆì´í„´ì‹œ ëª¨ë‹ˆí„°ë§ì— ì‚¬ìš©í•¨.
OTLP(OpenTelemetry Protocol)ë¥¼ ì‚¬ìš©í•¨ <br>

AKS+Istioì—ì„œëŠ” **Envoyê°€ ìƒì„±í•œ ìŠ¤íŒ¬ì„ OTLPë¡œ Collector â†’ (Tempo/Jaeger)** ë¡œ ì „ì†¡í•˜ê³ , Grafana/Kialiì—ì„œ ì¡°íšŒí•˜ëŠ” í˜•íƒœê°€ í‘œì¤€ì ì…ë‹ˆë‹¤. ì´ë•Œ ë©”íŠ¸ë¦­(Alerting)ì€ Prometheus/PromQLë¡œ, íŠ¸ë ˆì´ìŠ¤ íƒìƒ‰ì€ TraceQL/Jaeger UIë¡œ ì—­í• ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤. <br>

**ìŠ¤íŒ¬(Span)** ì€ í•˜ë‚˜ì˜ íŠ¸ë ˆì´ìŠ¤(trace)ë¥¼ êµ¬ì„±í•˜ëŠ” **ë‹¨ì¼ ì‘ì—… ë‹¨ìœ„**ë¡œ, *ì‘ì—… ì´ë¦„(operation name), ì‹œì‘Â·ì¢…ë£Œ ì‹œê°, ì§€ì†ì‹œê°„, ì†ì„±(attributes), ì´ë²¤íŠ¸(events), ë§í¬(links), ìƒíƒœ(status), ìŠ¤íŒ¬ ì»¨í…ìŠ¤íŠ¸(SpanContext)* ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ìŠ¤íŒ¬ë“¤ì€ **ë¶€ëª¨â€“ìì‹ ê´€ê³„**ë¡œ ì—°ê²°ë˜ì–´ íŠ¸ë ˆì´ìŠ¤(ìš”ì²­ì˜ ì „ì²´ ê²½ë¡œ)ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. <br>

ì• í”Œë¦¬ì¼€ì´ì…˜/í”„ë¡ì‹œ(Envoy)ê°€ ìŠ¤íŒ¬ ìƒì„± â†’ **OTLP/Zipkin** ë“±ìœ¼ë¡œ **OTel Collector** ìˆ˜ì§‘Â·ì „ì²˜ë¦¬ â†’ **íŠ¸ë ˆì´ì‹± ë°±ì—”ë“œ(Tempo/Jaeger)** ì €ì¥Â·ìƒ‰ì¸ â†’ Grafana/Jaeger UIì—ì„œ ìŠ¤íŒ¬Â·íŠ¸ë ˆì´ìŠ¤ ì¡°íšŒ(í•„ìš” ì‹œ **Prometheus ë©”íŠ¸ë¦­ê³¼ ìƒí˜¸ ì í”„**) <br>

**prometheus ì›ê²© ì“°ê¸°(ìˆ˜ì‹ ) enable**
```
# helm-charts/charts/kube-prometheus-stack/values.yaml
prometheus:
  prometheusSpec:
    enableRemoteWriteReceiver: true
```
<br>

**tempo ë°°í¬**
```
# tempo-values.yaml
metricsGenerator:
  enabled: true
  config:
    registry:
      collection_interval: 15s
    # Prometheus Remote-write ìˆ˜ì‹  ì—”ë“œí¬ì¸íŠ¸ë¡œ í‘¸ì‹œ
    storage:
      remote_write:
        - url: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/api/v1/writeextraArgs:
    - -config.expand-env=true
  extraEnv:
    - name: STORAGE_ACCOUNT_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: tempo-azure-credentials
          key: STORAGE_ACCOUNT_ACCESS_KEY
  

# Envoy/OTel Collectorê°€ ë³´ë‚¼ OTLP ìˆ˜ì‹  ì˜¤í”ˆ
traces:
  otlp:
    http:
      # -- Enable Tempo to ingest Open Telemetry HTTP traces
      enabled: false
      # -- HTTP receiver advanced config
      receiverConfig: {}
    grpc:
      # -- Enable Tempo to ingest Open Telemetry GRPC traces
      enabled: true
      # -- GRPC receiver advanced config
      receiverConfig: {}
      # -- Default OTLP gRPC port
      port: 4317

# í”„ë¡œì„¸ì„œ í™œì„±í™”ëŠ” overridesì—ì„œ ìˆ˜í–‰
overrides:
  defaults:
    metrics_generator:
      processors:
        - service-graphs
        - span-metrics
```
```
helm upgrade -i tempo grafana/tempo-distributed -n observability --create-namespace -f tempo-values.yaml
```
<br>

**íë¦„ê³¼ ì—­í• **
- ë©”ì‹œ íŠ¸ë˜í”½ â†’ (ìŠ¤íŒ¬) â†’ Tempo â†’ (ë©”íŠ¸ë¦­) â†’ Prometheus â†’ Grafana
- MeshConfig/Telemetry(ì •ì±…Â·ìƒ˜í”Œë§) â†’ Envoy(ìŠ¤íŒ¬ ìƒì„±/OTLP ì „ì†¡) â†’ Tempo(ì €ì¥/ë©”íŠ¸ë¦­ ìƒì„±) â†’ Prometheus(ì €ì¥) â†’ Grafana(ì‹œê°í™”).
<br>

**Istio â†’ Tempo(OTLP) ì „ì†¡ ì„¤ì •**
Mesh ë‹¨ì—ì„œ OTLP Providerë¥¼ Tempo Distributorë¡œ ì§€ì •í•˜ê³ , Telemetryë¡œ ìƒ˜í”Œë§ì„ ì¼­ë‹ˆë‹¤.
<br>

```
# istio-otlp-provider.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio
  namespace: istio-system
spec:
  meshConfig:
    defaultProviders:
      metrics: [prometheus]   # Prometheus ë©”íŠ¸ë¦­ í™œì„±í™” ì¤‘ìš”!
      tracing: [otlp]
    extensionProviders:
      - name: otlp
        opentelemetry:
          service: tempo-distributor.observability.svc.cluster.local
          port: 4317
```
```
istioctl install -f istio-otlp-provider.yaml -y
kubectl -n istio-system get cm istio -o jsonpath='{.data.mesh}' | sed -n '1,200p'
```
extensionProviders/opentelemetry, defaultProviders.tracingì— otlpê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
<br>

```
# istio-telemetry-traces.yaml
apiVersion: telemetry.istio.io/v1
kind: Telemetry
metadata:
  name: mesh-default
  namespace: istio-system
spec:
  tracing:
  - providers:
    - name: otlp
    randomSamplingPercentage: 100.0   # 1ì°¨ í™•ì¸ í›„ ë‚®ì¶”ê¸°
```
```
kubectl apply -f istio-telemetry-traces.yaml
kubectl -n istio-system get telemetry -o yaml
```
tracing.providers.name=otlp, randomSamplingPercentage í™•ì¸
<br>

**Telemetryë¥¼ Tempoì™€ í•¨ê»˜ ë°°í¬í•œ ì´ìœ **
**1. Telemetryì˜ ì—­í• **

- **Istioì˜ ê´€ì¸¡ ë°ì´í„° ì œì–´ ë ˆì´ì–´**
    
    Telemetry ë¦¬ì†ŒìŠ¤ëŠ” **íŠ¸ë ˆì´ì‹±, ë©”íŠ¸ë¦­, ì•¡ì„¸ìŠ¤ ë¡œê·¸**ê°€ ì–´ë–»ê²Œ ìˆ˜ì§‘ë˜ê³  ì–´ë–¤ ë°±ì—”ë“œë¡œ ì „ì†¡ë ì§€ ì •ì±…ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    
    ì˜ˆ: ìƒ˜í”Œë§ ë¹„ìœ¨(1%â†’5%), ì‚¬ìš©ì ì •ì˜ íƒœê·¸(env=prod), íŠ¹ì • ë©”íŠ¸ë¦­ ë¼ë²¨ ì¶”ê°€/ì œê±°.
    
- **í‘œì¤€í™”ëœ ì œì–´**
    
    ì›Œí¬ë¡œë“œ/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‹¨ìœ„ë¡œ ì„¸ë°€í•œ ì„¤ì •ì´ ê°€ëŠ¥í•´ ë¶ˆí•„ìš”í•œ ì‹ í˜¸ë¥¼ ì¤„ì´ê³  í•„ìš”í•œ ì‹ í˜¸ë§Œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    

---

**2. Tempoì˜ ì—­í• **

- **ë¶„ì‚° íŠ¸ë ˆì´ì‹± ë°±ì—”ë“œ**
    
    TempoëŠ” OpenTelemetry, Zipkin, Jaeger í¬ë§·ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” íŠ¸ë ˆì´ìŠ¤ë¥¼ ìˆ˜ì§‘Â·ì €ì¥í•˜ê³  Grafanaì—ì„œ TraceQLì„ í†µí•´ ì¡°íšŒí•©ë‹ˆë‹¤.
    
- **ëŒ€ê·œëª¨/ì €ë¹„ìš© ì„¤ê³„**
    
    íŠ¸ë ˆì´ìŠ¤ë¥¼ ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€ì— ì¥ê¸° ì €ì¥í•˜ë©°, ìš´ì˜ìê°€ í•„ìš”í•  ë•Œ ìƒì„¸ íŠ¸ë ˆì´ìŠ¤ë¥¼ ê²€ìƒ‰Â·ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.
    

---

**3. Telemetry + Tempoë¥¼ í•¨ê»˜ ë°°í¬í•˜ëŠ” ì´ìœ **

1. **ìƒì‚°â€“ì†Œë¹„ ì—°ê²°**
    - TelemetryëŠ” Istio í”„ë¡ì‹œê°€ ë§Œë“¤ì–´ë‚´ëŠ” íŠ¸ë ˆì´ìŠ¤(ìŠ¤íŒ¬)ë¥¼ ì–´ë–»ê²Œ ê°€ê³µÂ·ì „ì†¡í• ì§€ ì •ì˜í•©ë‹ˆë‹¤.
    - TempoëŠ” ì´ íŠ¸ë ˆì´ìŠ¤ë¥¼ ë°›ì•„ ì €ì¥Â·ì¡°íšŒí•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
        
        â†’ Telemetry ì—†ì´ëŠ” Tempoë¡œì˜ ì •í™•í•œ íŠ¸ë ˆì´ìŠ¤ ì†¡ì‹  ì œì–´ê°€ ì–´ë µìŠµë‹ˆë‹¤.
        
2. **ìƒ˜í”Œë§Â·íƒœê·¸ ì¼ê´€ì„± í™•ë³´**
    - ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ì„œë¹„ìŠ¤ë³„ë¡œ ìƒ˜í”Œë§ ë¹„ìœ¨ì´ë‚˜ íƒœê·¸ ì²´ê³„(ì„œë¹„ìŠ¤ëª…, í´ëŸ¬ìŠ¤í„°ëª…, í™˜ê²½ ë“±)ë¥¼ ë§ì¶”ì–´ì•¼ TraceQL ì¿¼ë¦¬ê°€ ì œëŒ€ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    - Telemetryê°€ ì´ë¥¼ í†µì¼í•´ Tempoì— ì „ë‹¬í•©ë‹ˆë‹¤.
3. **ë°±ì—”ë“œ ì—°ê³„ ìœ ì—°ì„±**
    - TelemetryëŠ” Tempoë¿ ì•„ë‹ˆë¼ Jaeger, Zipkin ê°™ì€ ë‹¤ë¥¸ íŠ¸ë ˆì´ì‹± ë°±ì—”ë“œë„ ë™ì‹œì— ì§€ì›í•©ë‹ˆë‹¤.
    - ë”°ë¼ì„œ Tempoë¡œ ì „í™˜Â·ë³‘í–‰ ìš´ì˜í•  ë•Œ ìœ ì—°ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **ìš´ì˜ ë¹„ìš© ìµœì í™”**
    - ë¶ˆí•„ìš”í•œ íŠ¸ë ˆì´ìŠ¤/ë©”íŠ¸ë¦­ì„ ì¤„ì´ê³ , ì¤‘ìš”í•œ ìš”ì²­ë§Œ Tempoì— ì €ì¥í•´ ìŠ¤í† ë¦¬ì§€ ë¹„ìš©ê³¼ ë¶„ì„ ë³µì¡ë„ë¥¼ ì¤„ì…ë‹ˆë‹¤.

---

**4. ê°„ë‹¨ ì•„í‚¤í…ì²˜ íë¦„**

```
[ì„œë¹„ìŠ¤ Pod/Envoy Proxy]
    â†“  (íŠ¸ë ˆì´ìŠ¤ ìƒì„±)
[Istio Telemetry ì •ì±… ì ìš©]
    â†“  (ìƒ˜í”Œë§/íƒœê·¸/ì „ì†¡ì œì–´, OTLP/Zipkin í¬ë§·)
[Collector or Direct Export]
    â†“
[Tempo Distributor â†’ Ingester â†’ Object Storage]
    â†“
[Grafana (TraceQL)ë¡œ ì¡°íšŒ ë° ë¶„ì„]

```

---

âœ… **ì •ë¦¬**: TelemetryëŠ” **â€œIstioê°€ ìƒì„±í•˜ëŠ” ê´€ì¸¡ ì‹ í˜¸ë¥¼ ì–´ë–»ê²Œ Tempo ê°™ì€ ë°±ì—”ë“œë¡œ ë³´ë‚¼ì§€ ì œì–´í•˜ëŠ” ì •ì±…â€**ì´ê³ , TempoëŠ” **â€œê·¸ ì‹ í˜¸ë¥¼ ë°›ì•„ ì €ì¥Â·ì¡°íšŒí•˜ëŠ” ë°±ì—”ë“œ ì‹œìŠ¤í…œâ€**ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‘˜ì€ **ë³´ì™„ ê´€ê³„**ì— ìˆìœ¼ë©°, **í•¨ê»˜ ë°°í¬í•´ì•¼ ìš´ì˜ì ì…ì¥ì—ì„œ ì™„ì „í•œ ê´€ì¸¡ ì²´ê³„**ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



## Loki

ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê·¸ ìˆ˜ì§‘ ë° ì €ì¥, Grafanaë¡œ ë¶„ì„
<br>